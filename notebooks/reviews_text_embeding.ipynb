{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import gc"
      ],
      "metadata": {
        "id": "vNMJKrpBXaHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GPU CHECK\n",
        "# ============================================================================\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cpu':\n",
        "    raise SystemExit(\"NO GPU! Go to Runtime > Change runtime type > Select GPU\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f\"\\n✓ GPU: {gpu_name}\")\n",
        "print(f\"✓ VRAM: {total_vram:.2f} GB\")\n",
        "\n",
        "# Aggressive batch sizing for speed\n",
        "if total_vram >= 15:\n",
        "    batch_size = 1024\n",
        "elif total_vram >= 12:\n",
        "    batch_size = 768\n",
        "else:\n",
        "    batch_size = 512\n",
        "\n",
        "print(f\"✓ Batch size: {batch_size}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-uhc025XdEG",
        "outputId": "ead3b00b-d25f-44b8-8b84-bfb90b2bd33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ GPU: Tesla T4\n",
            "✓ VRAM: 15.83 GB\n",
            "✓ Batch size: 1024\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#  DATA LOADING\n",
        "# ============================================================================\n",
        "filename = '/content/drive/MyDrive/reviews_complete.json'\n",
        "output_path = '/content/drive/MyDrive/embeddings/'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "print(f\"Loading from: {filename}\")\n",
        "print(\"Counting reviews...\")\n",
        "\n",
        "# Count lines efficiently\n",
        "with open(filename, 'r') as f:\n",
        "    num_lines = sum(1 for _ in f)\n",
        "print(f\"✓ Found {num_lines:,} reviews\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veRZK74KXiGW",
        "outputId": "8aefb43b-83a1-4057-dbe9-866f230dfb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading from: /content/drive/MyDrive/reviews_complete.json\n",
            "Counting reviews...\n",
            "✓ Found 2,516,821 reviews\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD MODEL FIRST\n",
        "# ============================================================================\n",
        "print(\"Loading embedding model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "model.eval()  # Set to eval mode for faster inference\n",
        "embedding_dim = model.get_sentence_embedding_dimension()\n",
        "print(f\"✓ Model loaded ({embedding_dim} dimensions)\\n\")\n",
        "\n",
        "# Estimate time\n",
        "est_speed = 2500  # T4 with batch_size=1024\n",
        "print(f\"⏱️  Estimated time: {(num_lines / est_speed) / 60:.1f} minutes\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PRE-ALLOCATE EVERYTHING\n",
        "# ============================================================================\n",
        "print(f\"Pre-allocating arrays...\")\n",
        "embeddings = np.zeros((num_lines, embedding_dim), dtype=np.float32)\n",
        "print(f\"✓ Embeddings array: {embeddings.nbytes/1e9:.2f} GB\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y6DdrDeXYb5",
        "outputId": "233dc306-14ae-4936-a331-909fdf4c6127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "✓ Model loaded (384 dimensions)\n",
            "\n",
            "⏱️  Estimated time: 16.8 minutes\n",
            "\n",
            "Pre-allocating arrays...\n",
            "✓ Embeddings array: 3.87 GB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STREAMING APPROACH - PROCESS WITHOUT LOADING FULL DF\n",
        "# ============================================================================\n",
        "print(\"Processing reviews in streaming mode...\")\n",
        "start_time = time.time()\n",
        "\n",
        "chunk_size = 50000\n",
        "current_idx = 0\n",
        "text_buffer = []\n",
        "buffer_size = batch_size * 10\n",
        "\n",
        "# Store metadata separately as we go\n",
        "metadata_list = []\n",
        "\n",
        "try:\n",
        "    with tqdm(total=num_lines, desc=\"Processing\") as pbar:\n",
        "        for chunk in pd.read_json(filename, lines=True, chunksize=chunk_size):\n",
        "            # Extract only what we need\n",
        "            texts = chunk['text'].fillna('').tolist()\n",
        "\n",
        "            # Save metadata (only essential columns)\n",
        "            meta_chunk = chunk[['review_id']].copy()\n",
        "            for col in ['business_id', 'stars', 'sentiment', 'date', 'user_id']:\n",
        "                if col in chunk.columns:\n",
        "                    meta_chunk[col] = chunk[col]\n",
        "            metadata_list.append(meta_chunk)\n",
        "\n",
        "            # Process texts in batches\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "                # Encode batch\n",
        "                batch_embeddings = model.encode(\n",
        "                    batch_texts,\n",
        "                    batch_size=batch_size,\n",
        "                    show_progress_bar=False,\n",
        "                    convert_to_numpy=True,\n",
        "                    normalize_embeddings=True,\n",
        "                    convert_to_tensor=False  # Stay in numpy\n",
        "                )\n",
        "\n",
        "                # Store directly\n",
        "                batch_end = current_idx + len(batch_texts)\n",
        "                embeddings[current_idx:batch_end] = batch_embeddings\n",
        "                current_idx = batch_end\n",
        "\n",
        "                pbar.update(len(batch_texts))\n",
        "\n",
        "                # GPU cleanup every 20 batches\n",
        "                if (current_idx // batch_size) % 20 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Clear chunk from memory\n",
        "            del chunk, texts\n",
        "            gc.collect()\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n✓ Complete! {current_idx:,} embeddings in {elapsed/60:.1f} min\")\n",
        "    print(f\"✓ Speed: {current_idx/elapsed:.0f} reviews/sec\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n ERROR: {e}\")\n",
        "    raise\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE EMBEDDINGS IMMEDIATELY\n",
        "# ============================================================================\n",
        "print(\"Saving embeddings...\")\n",
        "np.save(f'{output_path}review_embeddings.npy', embeddings)\n",
        "print(f\"✓ Saved: review_embeddings.npy ({embeddings.nbytes/1e9:.2f} GB)\")\n",
        "\n",
        "# Free embedding memory\n",
        "del embeddings\n",
        "gc.collect()\n",
        "\n",
        "# ============================================================================\n",
        "# SAVE METADATA\n",
        "# ============================================================================\n",
        "print(\"Saving metadata...\")\n",
        "metadata = pd.concat(metadata_list, ignore_index=True)\n",
        "del metadata_list\n",
        "gc.collect()\n",
        "\n",
        "metadata.to_parquet(f'{output_path}review_metadata.parquet', index=False)\n",
        "print(f\"✓ Saved: review_metadata.parquet ({len(metadata):,} rows)\\n\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ ALL DONE!\")\n",
        "print(f\"✓ Total time: {elapsed/60:.1f} minutes\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGAmruIUjhc",
        "outputId": "850fc8b9-3a7a-4ccf-e942-02666254c7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing reviews in streaming mode...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|██████████| 2516821/2516821 [1:58:42<00:00, 353.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Complete! 2,516,821 embeddings in 118.7 min\n",
            "✓ Speed: 353 reviews/sec\n",
            "\n",
            "Saving embeddings...\n",
            "✓ Saved: review_embeddings.npy (3.87 GB)\n",
            "Saving metadata...\n",
            "✓ Saved: review_metadata.parquet (2,516,821 rows)\n",
            "\n",
            "======================================================================\n",
            "✓ ALL DONE!\n",
            "✓ Total time: 118.7 minutes\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# QUICK TEST\n",
        "# ============================================================================\n",
        "print(\"Quick test:\")\n",
        "test = np.load(f'{output_path}review_embeddings.npy')\n",
        "meta = pd.read_parquet(f'{output_path}review_metadata.parquet')\n",
        "print(f\"✓ Embeddings: {test.shape}\")\n",
        "print(f\"✓ Metadata: {meta.shape}\")\n",
        "print(f\"✓ Aligned: {len(test) == len(meta)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"SUCCESS! Files saved to Google Drive:\")\n",
        "print(f\"  {output_path}review_embeddings.npy\")\n",
        "print(f\"  {output_path}review_metadata.parquet\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cInAlrv0VHmJ",
        "outputId": "43713cc8-a309-45eb-8335-47f0800591ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quick test:\n",
            "✓ Embeddings: (2516821, 384)\n",
            "✓ Metadata: (2516821, 5)\n",
            "✓ Aligned: True\n",
            "\n",
            "======================================================================\n",
            "SUCCESS! Files saved to Google Drive:\n",
            "  /content/drive/MyDrive/embeddings/review_embeddings.npy\n",
            "  /content/drive/MyDrive/embeddings/review_metadata.parquet\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}